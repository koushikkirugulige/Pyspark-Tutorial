{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- EventDate: string (nullable = true)\n",
      "\n",
      "+---+----------+\n",
      "| ID| EventDate|\n",
      "+---+----------+\n",
      "|123|04/05/2020|\n",
      "|124|  4/5/2020|\n",
      "|125| 04/5/2020|\n",
      "|126| 4/05/2020|\n",
      "+---+----------+\n",
      "\n",
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- EventDate: date (nullable = true)\n",
      "\n",
      "+---+----------+\n",
      "| ID| EventDate|\n",
      "+---+----------+\n",
      "|123|2020-04-05|\n",
      "|124|2020-04-05|\n",
      "|125|2020-04-05|\n",
      "|126|2020-04-05|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def to_date_df(df, fmt, fld):\n",
    "    return df.withColumn(fld, to_date(fld, fmt))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[3]\") \\\n",
    "        .appName(\"RowDemo\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "    my_schema = StructType([\n",
    "        StructField(\"ID\", StringType()),\n",
    "        StructField(\"EventDate\", StringType())])\n",
    "\n",
    "    my_rows = [Row(\"123\", \"04/05/2020\"), Row(\"124\", \"4/5/2020\"), Row(\"125\", \"04/5/2020\"), Row(\"126\", \"4/05/2020\")]\n",
    "    my_rdd = spark.sparkContext.parallelize(my_rows, 2)\n",
    "    my_df = spark.createDataFrame(my_rdd, my_schema)\n",
    "\n",
    "    my_df.printSchema()\n",
    "    my_df.show()\n",
    "    new_df = to_date_df(my_df, \"M/d/y\", \"EventDate\")\n",
    "    new_df.printSchema()\n",
    "    new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Unstructured File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .appName(\"RowDemo\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "df = spark.read.text('apache_log.txt')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ip: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- request: string (nullable = true)\n",
      " |-- referrer: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = r'^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\S+) \"(\\S+)\" \"([^\"]*)'\n",
    "logs_df = df.select(regexp_extract('value',log_reg,1).alias('ip'),\n",
    "                    regexp_extract('value',log_reg,4).alias('date'),\n",
    "                    regexp_extract('value',log_reg,6).alias('request'),\n",
    "                    regexp_extract('value',log_reg,10).alias('referrer'))\n",
    "logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+\n",
      "|          ip|                date|             request|            referrer|\n",
      "+------------+--------------------+--------------------+--------------------+\n",
      "|83.149.9.216|17/May/2015:10:05...|/presentations/lo...|http://semicomple...|\n",
      "|83.149.9.216|17/May/2015:10:05...|/presentations/lo...|http://semicomple...|\n",
      "|83.149.9.216|17/May/2015:10:05...|/presentations/lo...|http://semicomple...|\n",
      "|83.149.9.216|17/May/2015:10:05...|/presentations/lo...|http://semicomple...|\n",
      "|83.149.9.216|17/May/2015:10:05...|/presentations/lo...|http://semicomple...|\n",
      "+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            referrer|count|\n",
      "+--------------------+-----+\n",
      "|http://www.semico...| 3038|\n",
      "|http://semicomple...| 2001|\n",
      "|http://www.google...|  123|\n",
      "|https://www.googl...|  105|\n",
      "|http://stackoverf...|   34|\n",
      "|http://www.google.fr|   31|\n",
      "|http://s-chassis....|   29|\n",
      "| http://logstash.net|   28|\n",
      "|http://www.google.es|   25|\n",
      "|https://www.googl...|   23|\n",
      "|http://www.s-chas...|   22|\n",
      "|http://www.google.de|   18|\n",
      "|https://www.googl...|   15|\n",
      "|http://www.google...|   14|\n",
      "|https://www.googl...|   13|\n",
      "|https://www.googl...|   13|\n",
      "|http://www.google...|   12|\n",
      "| http://tuxradar.com|   12|\n",
      "|http://r.duckduck...|   11|\n",
      "|http://en.wikiped...|   10|\n",
      "|http://kufli.blog...|   10|\n",
      "|https://www.googl...|    9|\n",
      "|http://unix.stack...|    8|\n",
      "|http://www.google.it|    7|\n",
      "|https://www.googl...|    7|\n",
      "|http://www.google.ru|    6|\n",
      "|http://www.google.ca|    6|\n",
      "|https://www.googl...|    6|\n",
      "| http://www.bing.com|    6|\n",
      "|http://www.google.dk|    6|\n",
      "|https://www.googl...|    5|\n",
      "|http://www.linuxq...|    5|\n",
      "|https://www.googl...|    5|\n",
      "|https://www.googl...|    5|\n",
      "|http://kufli.blog...|    5|\n",
      "|http://www.google...|    4|\n",
      "|http://www.google...|    4|\n",
      "|http://ubuntuforu...|    4|\n",
      "|http://www.google.fi|    4|\n",
      "|http://www.google.nl|    4|\n",
      "|https://www.googl...|    4|\n",
      "|http://www.google...|    4|\n",
      "|http://www.linux....|    4|\n",
      "|https://www.googl...|    4|\n",
      "|https://www.googl...|    4|\n",
      "|https://duckduckg...|    3|\n",
      "| http://keepass.info|    3|\n",
      "|http://zolotoy-li...|    3|\n",
      "|http://www.google...|    3|\n",
      "|http://www.logsta...|    3|\n",
      "|http://www.comple...|    3|\n",
      "|http://askubuntu.com|    3|\n",
      "|https://www.googl...|    3|\n",
      "|http://znakomstva...|    3|\n",
      "|https://www.googl...|    3|\n",
      "|http://kherson-ap...|    3|\n",
      "| http://sofit-dmd.ru|    3|\n",
      "| http://suckless.org|    3|\n",
      "|http://xn--90adhh...|    3|\n",
      "|https://www.googl...|    3|\n",
      "|http://danceunive...|    3|\n",
      "|http://www.baidu.com|    3|\n",
      "|  http://avtoads.net|    3|\n",
      "|https://www.googl...|    3|\n",
      "|http://blackwitch...|    3|\n",
      "|http://ru.drugspo...|    3|\n",
      "|http://www.am-se.com|    3|\n",
      "|         http://t.co|    3|\n",
      "|http://mishura-op...|    3|\n",
      "|http://blog.float.tw|    3|\n",
      "|https://www.googl...|    2|\n",
      "|http://www.google...|    2|\n",
      "|http://manpages.u...|    2|\n",
      "|http://search.dau...|    2|\n",
      "|https://www.googl...|    2|\n",
      "|https://www.googl...|    2|\n",
      "|http://www.google...|    2|\n",
      "|http://www.google.lv|    2|\n",
      "|http://community....|    2|\n",
      "|http://www.dual-b...|    2|\n",
      "|http://www.google...|    2|\n",
      "|http://kufli.blog...|    2|\n",
      "|https://www.googl...|    2|\n",
      "|http://translate....|    2|\n",
      "|https://www.googl...|    2|\n",
      "|https://www.googl...|    2|\n",
      "|http://superuser.com|    2|\n",
      "|http://www.google...|    2|\n",
      "|https://www.googl...|    2|\n",
      "|https://encrypted...|    2|\n",
      "|http://www.google...|    2|\n",
      "|http://www.alittl...|    2|\n",
      "|    http://yandex.ru|    2|\n",
      "|https://www.googl...|    2|\n",
      "|http://ijavascrip...|    1|\n",
      "|http://www.google...|    1|\n",
      "|https://www.googl...|    1|\n",
      "|http://www.raspbe...|    1|\n",
      "|http://kufli.blog...|    1|\n",
      "|   http://rungie.com|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.where(\"referrer != '-'\").withColumn('referrer',substring_index('referrer',\"/\",3)).groupBy('referrer').count().orderBy(col('count').desc()).show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns Tranforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlinesDF = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"inferSchema\",\"true\") \\\n",
    ".load(\"flights.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- #Airline: string (nullable = true)\n",
      " |--  Departure: string (nullable = true)\n",
      " |--  Arrival: string (nullable = true)\n",
      " |--  Plane: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlinesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+-----------+\n",
      "|#Airline| Departure| Arrival|      Plane|\n",
      "+--------+----------+--------+-----------+\n",
      "|      NF|       NUS|     VLI|YN2;DHT;BNI|\n",
      "|      NF|       SON|     LNE|YN2;DHT;BNI|\n",
      "|      NF|       CCV|     NUS|    YN2;DHT|\n",
      "|      NF|       VLI|     NUS|    YN2;DHT|\n",
      "|      NF|       AUY|     TAH|    YN2;BNI|\n",
      "|      NF|       FTA|     TAH|    YN2;BNI|\n",
      "|      NF|       LNE|     SON|    YN2;BNI|\n",
      "|      NF|       TAH|     AUY|    YN2;BNI|\n",
      "|      NF|       TAH|     AWD|    YN2;BNI|\n",
      "|      NF|       AWD|     FTA|        YN2|\n",
      "+--------+----------+--------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlinesDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------------+\n",
      "|#Airline|Arrival|        concCol|\n",
      "+--------+-------+---------------+\n",
      "|      NF|    VLI|VLI-YN2;DHT;BNI|\n",
      "|      NF|    LNE|LNE-YN2;DHT;BNI|\n",
      "|      NF|    NUS|    NUS-YN2;DHT|\n",
      "|      NF|    NUS|    NUS-YN2;DHT|\n",
      "|      NF|    TAH|    TAH-YN2;BNI|\n",
      "|      NF|    TAH|    TAH-YN2;BNI|\n",
      "|      NF|    SON|    SON-YN2;BNI|\n",
      "|      NF|    AUY|    AUY-YN2;BNI|\n",
      "|      NF|    AWD|    AWD-YN2;BNI|\n",
      "|      NF|    FTA|        FTA-YN2|\n",
      "+--------+-------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlinesDF.select(column('#Airline'),column('Arrival'),expr(\"concat(Arrival,'-',Plane) as concCol\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+\n",
      "|#Airline|Arrival|concCol|\n",
      "+--------+-------+-------+\n",
      "|      NF|    VLI|    YN2|\n",
      "|      NF|    LNE|    YN2|\n",
      "|      NF|    NUS|    YN2|\n",
      "|      NF|    NUS|    YN2|\n",
      "|      NF|    TAH|    YN2|\n",
      "|      NF|    TAH|    YN2|\n",
      "|      NF|    SON|    YN2|\n",
      "|      NF|    AUY|    YN2|\n",
      "|      NF|    AWD|    YN2|\n",
      "|      NF|    FTA|    YN2|\n",
      "+--------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airlinesDF.select(column('#Airline'),column('Arrival'),expr(\"split(Plane,';')[0] as concCol\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Timestamp: timestamp (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- self_employed: string (nullable = true)\n",
      " |-- family_history: string (nullable = true)\n",
      " |-- treatment: string (nullable = true)\n",
      " |-- work_interfere: string (nullable = true)\n",
      " |-- no_employees: string (nullable = true)\n",
      " |-- remote_work: string (nullable = true)\n",
      " |-- tech_company: string (nullable = true)\n",
      " |-- benefits: string (nullable = true)\n",
      " |-- care_options: string (nullable = true)\n",
      " |-- wellness_program: string (nullable = true)\n",
      " |-- seek_help: string (nullable = true)\n",
      " |-- anonymity: string (nullable = true)\n",
      " |-- leave: string (nullable = true)\n",
      " |-- mental_health_consequence: string (nullable = true)\n",
      " |-- phys_health_consequence: string (nullable = true)\n",
      " |-- coworkers: string (nullable = true)\n",
      " |-- supervisor: string (nullable = true)\n",
      " |-- mental_health_interview: string (nullable = true)\n",
      " |-- phys_health_interview: string (nullable = true)\n",
      " |-- mental_vs_physical: string (nullable = true)\n",
      " |-- obs_consequence: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "surveyDF = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"inferSchema\",\"true\") \\\n",
    ".load(\"survey.csv\")\n",
    "\n",
    "surveyDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
      "|          Timestamp|Age|Gender|       Country|state|self_employed|family_history|treatment|work_interfere|  no_employees|remote_work|tech_company|  benefits|care_options|wellness_program| seek_help| anonymity|             leave|mental_health_consequence|phys_health_consequence|   coworkers|supervisor|mental_health_interview|phys_health_interview|mental_vs_physical|obs_consequence|comments|\n",
      "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
      "|2014-08-27 11:29:31| 37|Female| United States|   IL|           NA|            No|      Yes|         Often|          6-25|         No|         Yes|       Yes|    Not sure|              No|       Yes|       Yes|     Somewhat easy|                       No|                     No|Some of them|       Yes|                     No|                Maybe|               Yes|             No|      NA|\n",
      "|2014-08-27 11:29:37| 44|     M| United States|   IN|           NA|            No|       No|        Rarely|More than 1000|         No|          No|Don't know|          No|      Don't know|Don't know|Don't know|        Don't know|                    Maybe|                     No|          No|        No|                     No|                   No|        Don't know|             No|      NA|\n",
      "|2014-08-27 11:29:44| 32|  Male|        Canada|   NA|           NA|            No|       No|        Rarely|          6-25|         No|         Yes|        No|          No|              No|        No|Don't know|Somewhat difficult|                       No|                     No|         Yes|       Yes|                    Yes|                  Yes|                No|             No|      NA|\n",
      "|2014-08-27 11:29:46| 31|  Male|United Kingdom|   NA|           NA|           Yes|      Yes|         Often|        26-100|         No|         Yes|        No|         Yes|              No|        No|        No|Somewhat difficult|                      Yes|                    Yes|Some of them|        No|                  Maybe|                Maybe|                No|            Yes|      NA|\n",
      "|2014-08-27 11:30:22| 31|  Male| United States|   TX|           NA|            No|       No|         Never|       100-500|        Yes|         Yes|       Yes|          No|      Don't know|Don't know|Don't know|        Don't know|                       No|                     No|Some of them|       Yes|                    Yes|                  Yes|        Don't know|             No|      NA|\n",
      "|2014-08-27 11:31:22| 33|  Male| United States|   TN|           NA|           Yes|       No|     Sometimes|          6-25|         No|         Yes|       Yes|    Not sure|              No|Don't know|Don't know|        Don't know|                       No|                     No|         Yes|       Yes|                     No|                Maybe|        Don't know|             No|      NA|\n",
      "|2014-08-27 11:31:50| 35|Female| United States|   MI|           NA|           Yes|      Yes|     Sometimes|           1-5|        Yes|         Yes|        No|          No|              No|        No|        No|Somewhat difficult|                    Maybe|                  Maybe|Some of them|        No|                     No|                   No|        Don't know|             No|      NA|\n",
      "|2014-08-27 11:32:05| 39|     M|        Canada|   NA|           NA|            No|       No|         Never|           1-5|        Yes|         Yes|        No|         Yes|              No|        No|       Yes|        Don't know|                       No|                     No|          No|        No|                     No|                   No|                No|             No|      NA|\n",
      "|2014-08-27 11:32:39| 42|Female| United States|   IL|           NA|           Yes|      Yes|     Sometimes|       100-500|         No|         Yes|       Yes|         Yes|              No|        No|        No|    Very difficult|                    Maybe|                     No|         Yes|       Yes|                     No|                Maybe|                No|             No|      NA|\n",
      "|2014-08-27 11:32:43| 23|  Male|        Canada|   NA|           NA|            No|       No|         Never|        26-100|         No|         Yes|Don't know|          No|      Don't know|Don't know|Don't know|        Don't know|                       No|                     No|         Yes|       Yes|                  Maybe|                Maybe|               Yes|             No|      NA|\n",
      "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "surveyDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix gender column\n",
    "import re\n",
    "def parse_gender(gender):\n",
    "    female_pattern = r\"^f$|f.m|w.m\"\n",
    "    male_pattern = r\"^m$|ma|m.l\"\n",
    "    if re.search(female_pattern, gender.lower()):\n",
    "        return \"Female\"\n",
    "    if re.search(male_pattern, gender.lower()):\n",
    "        return \"Male\"\n",
    "    else:\n",
    "        return \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#register UDF function\n",
    "parse_gender_udf = udf(parse_gender,StringType())\n",
    "surveyDF2 = surveyDF.withColumn('Gender',parse_gender_udf('Gender'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Gender|\n",
      "+------+\n",
      "|Female|\n",
      "|  Male|\n",
      "|  Male|\n",
      "|  Male|\n",
      "|  Male|\n",
      "|  Male|\n",
      "|Female|\n",
      "|  Male|\n",
      "|Female|\n",
      "|  Male|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "surveyDF2.select('Gender').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .appName(\"RowDemo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data_list = [(\"Ravi\", \"28\", \"1\", \"2002\"),\n",
    "                 (\"Abdul\", \"23\", \"5\", \"81\"),  # 1981\n",
    "                 (\"John\", \"12\", \"12\", \"6\"),  # 2006\n",
    "                 (\"Rosy\", \"7\", \"8\", \"63\"),  # 1963\n",
    "                 (\"Abdul\", \"23\", \"5\", \"81\")]  # 1981\n",
    "\n",
    "raw_df = spark.createDataFrame(data_list).toDF(\"name\", \"day\", \"month\", \"year\").repartition(3)\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----------+\n",
      "| name|day|month|year|         id|\n",
      "+-----+---+-----+----+-----------+\n",
      "| Ravi| 28|    1|2002|          0|\n",
      "|Abdul| 23|    5|  81|          1|\n",
      "|Abdul| 23|    5|  81| 8589934592|\n",
      "| John| 12|   12|   6|17179869184|\n",
      "| Rosy|  7|    8|  63|17179869185|\n",
      "+-----+---+-----+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = raw_df.withColumn('id',monotonically_increasing_id())\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|day|month|year|\n",
      "+-----+---+-----+----+\n",
      "| Ravi| 28|    1|2002|\n",
      "|Abdul| 23|    5|1981|\n",
      "|Abdul| 23|    5|1981|\n",
      "| John| 12|   12|2006|\n",
      "| Rosy|  7|    8|1963|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = raw_df.withColumn('year',expr('CASE WHEN year <21 then cast(year+2000 as int) WHEN year < 100 then cast(1900 + year as int) else cast(year as int) end'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| name|       dob|\n",
      "+-----+----------+\n",
      "| Rosy|1963-08-07|\n",
      "|Abdul|1981-05-23|\n",
      "| Ravi|2002-01-28|\n",
      "| John|2006-12-12|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df2.withColumn('dob',to_date(expr(\"concat(day,'/',month,'/',year)\"),'d/M/y')).drop(\"day\",\"month\",\"year\") \\\n",
    "         .dropDuplicates([\"name\",\"dob\"]).sort(expr(\"dob desc\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .appName(\"Agg\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_df = spark.read.format('csv').option('InferSchema','true').option('header','true').load('invoices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "|   536365|     null|WHITE HANGING HEA...|       6|01-12-2010 8.26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|01-12-2010 8.26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|01-12-2010 8.26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|01-12-2010 8.26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|01-12-2010 8.28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|01-12-2010 8.28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|01-12-2010 8.34|     1.69|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inv_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------------+-------------+\n",
      "|Count *|TotQuantity|        MeanPrice|CountDistinct|\n",
      "+-------+-----------+-----------------+-------------+\n",
      "| 541909|    5176450|4.611113626088481|        25900|\n",
      "+-------+-----------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inv_df.select(f.count(\"*\").alias(\"Count *\"),\n",
    "             f.sum(\"Quantity\").alias('TotQuantity'),\n",
    "             f.mean(\"UnitPrice\").alias('MeanPrice'),\n",
    "             countDistinct('InvoiceNo').alias('CountDistinct')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-----------------+\n",
      "| count|sum(Quantity)|   avg(UnitPrice)|\n",
      "+------+-------------+-----------------+\n",
      "|541909|      5176450|4.611113626086849|\n",
      "+------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inv_df.selectExpr(\"count(1) as `count`\",\n",
    "                 \"sum(Quantity)\",\n",
    "                 \"avg(UnitPrice) \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-----------+\n",
      "|       Country|InvoiceNo|TotQuantity|\n",
      "+--------------+---------+-----------+\n",
      "|United Kingdom|   536446|        329|\n",
      "|United Kingdom|   536508|        216|\n",
      "|United Kingdom|   537018|         -3|\n",
      "|United Kingdom|   537401|        -24|\n",
      "|United Kingdom|   537811|         74|\n",
      "|United Kingdom|  C537824|         -2|\n",
      "|United Kingdom|   538895|        370|\n",
      "|United Kingdom|   540453|        341|\n",
      "|United Kingdom|   541291|        217|\n",
      "|United Kingdom|   542551|         -1|\n",
      "|United Kingdom|   542576|         -1|\n",
      "|United Kingdom|   542628|          9|\n",
      "|United Kingdom|   542886|        199|\n",
      "|United Kingdom|   542907|         75|\n",
      "|United Kingdom|   543131|        134|\n",
      "|United Kingdom|   543189|        102|\n",
      "|United Kingdom|   543265|         -4|\n",
      "|        Cyprus|   544574|        173|\n",
      "|United Kingdom|   545077|         24|\n",
      "|United Kingdom|   545300|        116|\n",
      "+--------------+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#group by \n",
    "summary_df = inv_df.groupBy('Country','InvoiceNo') \\\n",
    ".agg(f.sum(\"Quantity\").alias('TotQuantity')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = inv_df.withColumn('InvoiceDate',expr(\"split(InvoiceDate,' ')[0]\"))\n",
    "\n",
    "temp_df1 = temp_df.withColumn('InvoiceDate',f.to_date('InvoiceDate',\"dd-MM-yyyy\"))\n",
    "\n",
    "fin_df = temp_df1.withColumn('WeekNo',f.weekofyear('InvoiceDate'))\n",
    "\n",
    "sum_df =fin_df.groupBy('Country','WeekNo').agg(f.countDistinct(\"InvoiceNo\").alias('NumInvoices'),\n",
    "                                               f.sum('Quantity').alias('TotalQuantity'),\n",
    "                                               f.expr('round(sum(UnitPrice * Quantity),3) as InvoiceValue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+-------------+------------+\n",
      "|Country|WeekNo|NumInvoices|TotalQuantity|InvoiceValue|\n",
      "+-------+------+-----------+-------------+------------+\n",
      "|   EIRE|     1|          2|          -17|       25.33|\n",
      "|   EIRE|     2|          2|         6194|    16759.72|\n",
      "|   EIRE|     3|          2|         1619|     3222.42|\n",
      "|   EIRE|     4|          3|          892|     1649.05|\n",
      "|   EIRE|     5|          4|          812|     1830.43|\n",
      "|   EIRE|     6|          1|           -4|       -25.8|\n",
      "|   EIRE|     7|          6|         4052|     7927.03|\n",
      "|   EIRE|     8|          4|          468|      -57.52|\n",
      "|   EIRE|     9|          4|         1514|     3716.44|\n",
      "|   EIRE|    10|          8|         3416|     6162.02|\n",
      "|   EIRE|    11|         10|         2864|     6533.19|\n",
      "|   EIRE|    12|          3|          644|     1141.03|\n",
      "|   EIRE|    13|          4|         1837|     2637.74|\n",
      "|   EIRE|    14|          5|         2284|     3987.52|\n",
      "|   EIRE|    15|          3|          741|     1445.27|\n",
      "|   EIRE|    16|          1|          234|      367.62|\n",
      "|   EIRE|    17|          1|          163|       362.9|\n",
      "|   EIRE|    18|          6|         2864|     6252.83|\n",
      "|   EIRE|    19|          4|          488|     1414.23|\n",
      "|   EIRE|    20|         11|         3945|      8778.6|\n",
      "+-------+------+-----------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sum_df.filter(sum_df.Country == \"EIRE\").orderBy('WeekNo').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = spark.read.parquet('summary.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+-------------+------------+\n",
      "|  Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|\n",
      "+---------+----------+-----------+-------------+------------+\n",
      "|    Spain|        49|          1|           67|      174.72|\n",
      "|  Germany|        48|         11|         1795|     3309.75|\n",
      "|Lithuania|        48|          3|          622|     1598.06|\n",
      "|  Germany|        49|         12|         1852|     4521.39|\n",
      "|  Bahrain|        51|          1|           54|      205.74|\n",
      "|  Iceland|        49|          1|          319|      711.79|\n",
      "|     EIRE|        51|          5|           95|      276.84|\n",
      "|Australia|        50|          2|          133|      387.95|\n",
      "|    Italy|        49|          1|           -2|       -17.0|\n",
      "|     EIRE|        49|          5|         1280|      3284.1|\n",
      "+---------+----------+-----------+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+-----------+-------------+------------+------------------+\n",
      "|        Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|      RunningTotal|\n",
      "+---------------+----------+-----------+-------------+------------+------------------+\n",
      "|      Australia|        48|          1|          107|      358.25|            358.25|\n",
      "|      Australia|        49|          1|          214|       258.9|            617.15|\n",
      "|      Australia|        50|          2|          133|      387.95|1005.0999999999999|\n",
      "|        Austria|        50|          2|            3|      257.04|            257.04|\n",
      "|        Bahrain|        51|          1|           54|      205.74|            205.74|\n",
      "|        Belgium|        48|          1|          528|       346.1|             346.1|\n",
      "|        Belgium|        50|          2|          285|      625.16|            971.26|\n",
      "|        Belgium|        51|          2|          942|      838.65|1809.9099999999999|\n",
      "|Channel Islands|        49|          1|           80|      363.53|            363.53|\n",
      "|         Cyprus|        50|          1|          917|     1590.82|           1590.82|\n",
      "|        Denmark|        49|          1|          454|      1281.5|            1281.5|\n",
      "|           EIRE|        48|          7|         2822|     3147.23|           3147.23|\n",
      "|           EIRE|        49|          5|         1280|      3284.1|           6431.33|\n",
      "|           EIRE|        50|          5|         1184|     2321.78|           8753.11|\n",
      "|           EIRE|        51|          5|           95|      276.84|           9029.95|\n",
      "|        Finland|        50|          1|         1254|       892.8|             892.8|\n",
      "|         France|        48|          4|         1299|     2808.16|           2808.16|\n",
      "|         France|        49|          9|         2303|     4527.01|           7335.17|\n",
      "|         France|        50|          6|          529|      537.32|           7872.49|\n",
      "|         France|        51|          5|          847|     1702.87|           9575.36|\n",
      "+---------------+----------+-----------+-------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_total_window = Window.partitionBy(\"Country\") \\\n",
    "    .orderBy('Weeknumber') \\\n",
    "    .rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "summary_df.withColumn(\"RunningTotal\",f.sum('InvoiceValue').over(running_total_window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .appName(\"Join\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_list = [(\"01\", \"02\", 350, 1),\n",
    "               (\"01\", \"04\", 580, 1),\n",
    "               (\"01\", \"07\", 320, 2),\n",
    "               (\"02\", \"03\", 450, 1),\n",
    "               (\"02\", \"06\", 220, 1),\n",
    "               (\"03\", \"01\", 195, 1),\n",
    "               (\"04\", \"09\", 270, 3),\n",
    "               (\"04\", \"08\", 410, 2),\n",
    "               (\"05\", \"02\", 350, 1)]\n",
    "\n",
    "order_df = spark.createDataFrame(orders_list).toDF(\"order_id\", \"prod_id\", \"unit_price\", \"qty\")\n",
    "\n",
    "product_list = [(\"01\", \"Scroll Mouse\", 250, 20),\n",
    "                (\"02\", \"Optical Mouse\", 350, 20),\n",
    "                (\"03\", \"Wireless Mouse\", 450, 50),\n",
    "                (\"04\", \"Wireless Keyboard\", 580, 50),\n",
    "                (\"05\", \"Standard Keyboard\", 360, 10),\n",
    "                (\"06\", \"16 GB Flash Storage\", 240, 100),\n",
    "                (\"07\", \"32 GB Flash Storage\", 320, 50),\n",
    "                (\"08\", \"64 GB Flash Storage\", 430, 25)]\n",
    "\n",
    "product_df = spark.createDataFrame(product_list).toDF(\"prod_id\", \"prod_name\", \"list_price\", \"qty\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+---+-------+-------------------+----------+\n",
      "|order_id|prod_id|unit_price|qty|prod_id|          prod_name|list_price|\n",
      "+--------+-------+----------+---+-------+-------------------+----------+\n",
      "|      03|     01|       195|  1|     01|       Scroll Mouse|       250|\n",
      "|      01|     02|       350|  1|     02|      Optical Mouse|       350|\n",
      "|      05|     02|       350|  1|     02|      Optical Mouse|       350|\n",
      "|      02|     03|       450|  1|     03|     Wireless Mouse|       450|\n",
      "|      01|     04|       580|  1|     04|  Wireless Keyboard|       580|\n",
      "|      02|     06|       220|  1|     06|16 GB Flash Storage|       240|\n",
      "|      01|     07|       320|  2|     07|32 GB Flash Storage|       320|\n",
      "|      04|     08|       410|  2|     08|64 GB Flash Storage|       430|\n",
      "+--------+-------+----------+---+-------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_df.join(product_df,order_df.prod_id == product_df.prod_id, 'inner') \\\n",
    "    .drop(product_df.qty).show()\n",
    "    #.select('product_df.order_id','product_df.unit_price') \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-25-709dff170757>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-709dff170757>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    .withColumn(\"prod_name\",expr(\"COALESCE(prod_name,order_df.prod_id)\"))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#left join\n",
    "\n",
    "order_df.join(product_df,order_df.prod_id == product_df.prod_id, 'left') \\\n",
    "    .drop('product_df.qty','product_df.prod_id') \\\n",
    "    .withColumn(\"prod_name\",expr(\"COALESCE(prod_name,order_df.prod_id)\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .appName(\"Join\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df1 = spark.read.json(\"d1\")\n",
    "f_df2 = spark.read.json(\"d2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for brodcast join\n",
    "#works when one df is small\n",
    "join_exp = f_df1.id == f_df2.id\n",
    "join_df = f_df1.join(broadcast(f_df2),join_exp,'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucket the dataframe in case of large dataset\n",
    "\n",
    "f_df1.coalesce(1).write \\\n",
    "    .bucketBy(3,\"id\").saveAsTable(\"flight_df1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df2.coalesce(1).write \\\n",
    "    .bucketBy(3,\"id\").saveAsTable(\"flight_df2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_df1 = spark.read.table(\"flight_df1\")\n",
    "\n",
    "db_df2 = spark.read.table('flight_df2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)\n",
    "\n",
    "\n",
    "join_exp = db_df1.id == db_df2.id\n",
    "join_df = db_df1.join(db_df2,join_exp,'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
